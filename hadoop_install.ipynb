{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Hadoop on Ubuntu\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Node Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Installing Java`\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ cd ~\n",
    "\n",
    "# Update the source list\n",
    "k@laptop:~$ sudo apt-get update\n",
    "\n",
    "# The OpenJDK project is the default version of Java \n",
    "# that is provided from a supported Ubuntu repository.\n",
    "k@laptop:~$ sudo apt-get install default-jdk\n",
    "\n",
    "k@laptop:~$ java -version\n",
    "java version \"1.7.0_65\"\n",
    "OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)\n",
    "OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Adding a dedicated Hadoop user`\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ sudo addgroup hadoop\n",
    "Adding group `hadoop' (GID 1002) ...\n",
    "Done.\n",
    "\n",
    "k@laptop:~$ sudo adduser --ingroup hadoop hduser\n",
    "Adding user `hduser' ...\n",
    "Adding new user `hduser' (1001) with group `hadoop' ...\n",
    "Creating home directory `/home/hduser' ...\n",
    "Copying files from `/etc/skel' ...\n",
    "Enter new UNIX password: \n",
    "Retype new UNIX password: \n",
    "passwd: password updated successfully\n",
    "Changing the user information for hduser\n",
    "Enter the new value, or press ENTER for the default\n",
    "\tFull Name []: \n",
    "\tRoom Number []: \n",
    "\tWork Phone []: \n",
    "\tHome Phone []: \n",
    "\tOther []: \n",
    "Is the information correct? [Y/n] Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Installing SSH`\n",
    "\n",
    "ssh has two main components:\n",
    "\n",
    "- ssh : The command we use to connect to remote machines - the client.\n",
    "- sshd : The daemon that is running on the server and allows clients to connect to the server.\n",
    "\n",
    "The ssh is pre-enabled on Linux, but in order to start sshd daemon, we need to install ssh first. Use this command to do that :\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ sudo apt-get install ssh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will install ssh on our machine. If we get something similar to the following, we can think it is setup properly:\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ which ssh\n",
    "/usr/bin/ssh\n",
    "\n",
    "k@laptop:~$ which sshd\n",
    "/usr/sbin/sshd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Create and Setup SSH Certificates`\n",
    "\n",
    "Hadoop requires SSH access to manage its nodes, i.e. remote machines plus our local machine. For our single-node setup of Hadoop, we therefore need to configure SSH access to localhost.\n",
    "\n",
    "So, we need to have SSH up and running on our machine and configured it to allow SSH public key authentication.\n",
    "\n",
    "Hadoop uses SSH (to access its nodes) which would normally require the user to enter a password. However, this requirement can be eliminated by creating and setting up SSH certificates using the following commands. If asked for a filename just leave it blank and press the enter key to continue.\n",
    "\n",
    "---\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ su hduser\n",
    "Password: \n",
    "k@laptop:~$ ssh-keygen -t rsa -P \"\"\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/hduser/.ssh/id_rsa): \n",
    "Created directory '/home/hduser/.ssh'.\n",
    "Your identification has been saved in /home/hduser/.ssh/id_rsa.\n",
    "Your public key has been saved in /home/hduser/.ssh/id_rsa.pub.\n",
    "The key fingerprint is:\n",
    "50:6b:f3:fc:0f:32:bf:30:79:c2:41:71:26:cc:7d:e3 hduser@laptop\n",
    "The key's randomart image is:\n",
    "+--[ RSA 2048]----+\n",
    "|        .oo.o    |\n",
    "|       . .o=. o  |\n",
    "|      . + .  o . |\n",
    "|       o =    E  |\n",
    "|        S +      |\n",
    "|         . +     |\n",
    "|          O +    |\n",
    "|           O o   |\n",
    "|            o..  |\n",
    "+-----------------+\n",
    "\n",
    "\n",
    "hduser@laptop:/home/k$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command adds the newly created key to the list of authorized keys so that Hadoop can use ssh without prompting for a password.\n",
    "\n",
    "We can check if ssh works:\n",
    "```Shell\n",
    "hduser@laptop:/home/k$ ssh localhost\n",
    "The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "ECDSA key fingerprint is e1:8b:a0:a5:75:ef:f4:b4:5e:a9:ed:be:64:be:5c:2f.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
    "Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-40-generic x86_64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Install Hadoop`\n",
    "```Shell\n",
    "hduser@laptop:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz\n",
    "hduser@laptop:~$ tar xvzf hadoop-2.6.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to move the Hadoop installation to the /usr/local/hadoop directory using the following command:\n",
    "\n",
    "```Shell\n",
    "hduser@laptop:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop\n",
    "[sudo] password for hduser: \n",
    "hduser is not in the sudoers file.  This incident will be reported.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops!... We got:\n",
    "\n",
    "```Shell\n",
    "\"hduser is not in the sudoers file. This incident will be reported.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error can be resolved by logging in as a root user, and then add hduser to sudo:\n",
    "\n",
    "```Bash\n",
    "hduser@laptop:~/hadoop-2.6.0$ su k\n",
    "Password: \n",
    "\n",
    "k@laptop:/home/hduser$ sudo adduser hduser sudo\n",
    "[sudo] password for k: \n",
    "Adding user `hduser' to group `sudo' ...\n",
    "Adding user hduser to group sudo\n",
    "Done.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the hduser has root priviledge, we can move the Hadoop installation to the /usr/local/hadoop directory without any problem:\n",
    "\n",
    "```Bash\n",
    "k@laptop:/home/hduser$ sudo su hduser\n",
    "\n",
    "hduser@laptop:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop \n",
    "hduser@laptop:~/hadoop-2.6.0$ sudo chown -R hduser:hadoop /usr/local/hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Configuration Files\n",
    "The following files will have to be modified to complete the Hadoop setup:\n",
    "\n",
    "- `~/.bashrc`\n",
    "- `/usr/local/hadoop/etc/hadoop/hadoop-env.sh`\n",
    "- `/usr/local/hadoop/etc/hadoop/core-site.xml`\n",
    "- `/usr/local/hadoop/etc/hadoop/mapred-site.xml.template`\n",
    "- `/usr/local/hadoop/etc/hadoop/hdfs-site.xml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before editing the .bashrc file in our home directory, we need to find the path where Java has been installed to set the JAVA_HOME environment variable using the following command:\n",
    "\n",
    "```Shell\n",
    "hduser@laptop update-alternatives --config java\n",
    "There is only one alternative in link group java (providing /usr/bin/java): /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n",
    "Nothing to configure.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.  ~/.bashrc`\n",
    "\n",
    "Now we can append the following to the end of ~/.bashrc:\n",
    "\n",
    "```Bash\n",
    "hduser@laptop:~$ vi ~/.bashrc\n",
    "\n",
    "#Please check your java home usually create a link at /usr/local/java\n",
    "export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n",
    "\n",
    "#HADOOP VARIABLES START\n",
    "export HADOOP_INSTALL=/usr/local/hadoop\n",
    "export PATH=$PATH:$HADOOP_INSTALL/bin\n",
    "export PATH=$PATH:$HADOOP_INSTALL/sbin\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_INSTALL\n",
    "export HADOOP_COMMON_HOME=$HADOOP_INSTALL\n",
    "export HADOOP_HDFS_HOME=$HADOOP_INSTALL\n",
    "export YARN_HOME=$HADOOP_INSTALL\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\n",
    "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib\"\n",
    "#HADOOP VARIABLES END\n",
    "\n",
    "hduser@laptop:~$ source ~/.bashrc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that the JAVA_HOME should be set as the path just before the '.../bin/':\n",
    "\n",
    "```Bash\n",
    "hduser@ubuntu-VirtualBox:~$ javac -version\n",
    "javac 1.7.0_75\n",
    "\n",
    "hduser@ubuntu-VirtualBox:~$ which javac\n",
    "/usr/bin/javac\n",
    "\n",
    "hduser@ubuntu-VirtualBox:~$ readlink -f /usr/bin/javac\n",
    "/usr/lib/jvm/java-7-openjdk-amd64/bin/javac\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2. /usr/local/hadoop/etc/hadoop/hadoop-env.sh`\n",
    "\n",
    "We need to set JAVA_HOME by modifying hadoop-env.sh file.\n",
    "\n",
    "```Shell\n",
    "hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n",
    "```\n",
    "\n",
    "Adding the above statement in the hadoop-env.sh file ensures that the value of JAVA_HOME variable will be available to Hadoop whenever it is started up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3. /usr/local/hadoop/etc/hadoop/core-site.xml:`\n",
    "\n",
    "The /usr/local/hadoop/etc/hadoop/core-site.xml file contains configuration properties that Hadoop uses when starting up. \n",
    "This file can be used to override the default settings that Hadoop starts with.\n",
    "\n",
    "```Shell\n",
    "hduser@laptop:~$ sudo mkdir -p /app/hadoop/tmp\n",
    "hduser@laptop:~$ sudo chown hduser:hadoop /app/hadoop/tmp\n",
    "```\n",
    "\n",
    "Open the file and enter the following in between the <configuration></configuration> tag:\n",
    "\n",
    "``\n",
    "hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/core-site.xml\n",
    "``\n",
    "\n",
    "```Xml\n",
    "<configuration>\n",
    " <property>\n",
    "  <name>hadoop.tmp.dir</name>\n",
    "  <value>/app/hadoop/tmp</value>\n",
    "  <description>A base for other temporary directories.</description>\n",
    " </property>\n",
    "\n",
    " <property>\n",
    "  <name>fs.default.name</name>\n",
    "  <value>hdfs://localhost:54310</value>\n",
    "  <description>The name of the default file system.  A URI whose\n",
    "  scheme and authority determine the FileSystem implementation.  The\n",
    "  uri's scheme determines the config property (fs.SCHEME.impl) naming\n",
    "  the FileSystem implementation class.  The uri's authority is used to\n",
    "  determine the host, port, etc. for a filesystem.</description>\n",
    " </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `4. /usr/local/hadoop/etc/hadoop/mapred-site.xml`\n",
    "\n",
    "By default, the `/usr/local/hadoop/etc/hadoop/ `folder contains \n",
    "`/usr/local/hadoop/etc/hadoop/mapred-site.xml.template`  file which has to be renamed/copied with the name `mapred-site.xml:`\n",
    "\n",
    "```Shell\n",
    "hduser@laptop:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml\n",
    "```\n",
    "The mapred-site.xml file is used to specify which framework is being used for MapReduce.\n",
    "We need to enter the following content in between the <configuration></configuration> tag:\n",
    "\n",
    "```Xml\n",
    "<configuration>\n",
    " <property>\n",
    "  <name>mapred.job.tracker</name>\n",
    "  <value>localhost:54311</value>\n",
    "  <description>The host and port that the MapReduce job tracker runs\n",
    "  at.  If \"local\", then jobs are run in-process as a single map\n",
    "  and reduce task.\n",
    "  </description>\n",
    " </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `5. /usr/local/hadoop/etc/hadoop/hdfs-site.xml`\n",
    "\n",
    "The /usr/local/hadoop/etc/hadoop/hdfs-site.xml file needs to be configured for each host in the cluster that is being used. \n",
    "It is used to specify the directories which will be used as the namenode and the datanode on that host.\n",
    "\n",
    "Before editing this file, we need to create two directories which will contain the namenode and the datanode for this Hadoop installation. \n",
    "This can be done using the following commands:\n",
    "\n",
    "```Shell\n",
    "hduser@laptop:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode\n",
    "hduser@laptop:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode\n",
    "hduser@laptop:~$ sudo chown -R hduser:hadoop /usr/local/hadoop_store\n",
    "```\n",
    "\n",
    "Open the file and enter the following content in between the <configuration></configuration> tag:\n",
    "\n",
    "```\n",
    "hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml\n",
    "```\n",
    "\n",
    "```Xml\n",
    "<configuration>\n",
    " <property>\n",
    "  <name>dfs.replication</name>\n",
    "  <value>1</value>\n",
    "  <description>Default block replication.\n",
    "  The actual number of replications can be specified when the file is created.\n",
    "  The default is used if replication is not specified in create time.\n",
    "  </description>\n",
    " </property>\n",
    " <property>\n",
    "   <name>dfs.namenode.name.dir</name>\n",
    "   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>\n",
    " </property>\n",
    " <property>\n",
    "   <name>dfs.datanode.data.dir</name>\n",
    "   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>\n",
    " </property>\n",
    "</configuration>m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Format File System`\n",
    "\n",
    "Now, the Hadoop file system needs to be formatted so that we can start to use it. The format command should be issued with write permission since it creates current directory \n",
    "under /usr/local/hadoop_store/hdfs/namenode folder:\n",
    "\n",
    "```Bash\n",
    "hduser@laptop:~$ hadoop namenode -format\n",
    "DEPRECATED: Use of this script to execute hdfs command is deprecated.\n",
    "Instead use the hdfs command for it.\n",
    "\n",
    "15/04/18 14:43:03 INFO namenode.NameNode: STARTUP_MSG: \n",
    "/************************************************************\n",
    "STARTUP_MSG: Starting NameNode\n",
    "STARTUP_MSG:   host = laptop/192.168.1.1\n",
    "STARTUP_MSG:   args = [-format]\n",
    "STARTUP_MSG:   version = 2.6.0\n",
    "STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop\n",
    "...\n",
    "STARTUP_MSG:   java = 1.7.0_65\n",
    "************************************************************/\n",
    "15/04/18 14:43:03 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
    "15/04/18 14:43:03 INFO namenode.NameNode: createNameNode [-format]\n",
    "15/04/18 14:43:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Formatting using clusterid: CID-e2f515ac-33da-45bc-8466-5b1100a2bf7f\n",
    "15/04/18 14:43:09 INFO namenode.FSNamesystem: No KeyProvider found.\n",
    "15/04/18 14:43:09 INFO namenode.FSNamesystem: fsLock is fair:true\n",
    "15/04/18 14:43:10 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n",
    "15/04/18 14:43:10 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: The block deletion will start around 2015 Apr 18 14:43:10\n",
    "15/04/18 14:43:10 INFO util.GSet: Computing capacity for map BlocksMap\n",
    "15/04/18 14:43:10 INFO util.GSet: VM type       = 64-bit\n",
    "15/04/18 14:43:10 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
    "15/04/18 14:43:10 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: minReplication             = 1\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
    "15/04/18 14:43:10 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
    "15/04/18 14:43:10 INFO namenode.FSNamesystem: fsOwner             = hduser (auth:SIMPLE)\n",
    "15/04/18 14:43:10 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
    "15/04/18 14:43:10 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
    "15/04/18 14:43:10 INFO namenode.FSNamesystem: HA Enabled: false\n",
    "15/04/18 14:43:10 INFO namenode.FSNamesystem: Append Enabled: true\n",
    "15/04/18 14:43:11 INFO util.GSet: Computing capacity for map INodeMap\n",
    "15/04/18 14:43:11 INFO util.GSet: VM type       = 64-bit\n",
    "15/04/18 14:43:11 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
    "15/04/18 14:43:11 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
    "15/04/18 14:43:11 INFO namenode.NameNode: Caching file names occuring more than 10 times\n",
    "15/04/18 14:43:11 INFO util.GSet: Computing capacity for map cachedBlocks\n",
    "15/04/18 14:43:11 INFO util.GSet: VM type       = 64-bit\n",
    "15/04/18 14:43:11 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
    "15/04/18 14:43:11 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
    "15/04/18 14:43:11 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
    "15/04/18 14:43:11 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n",
    "15/04/18 14:43:11 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n",
    "15/04/18 14:43:11 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
    "15/04/18 14:43:11 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
    "15/04/18 14:43:11 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
    "15/04/18 14:43:11 INFO util.GSet: VM type       = 64-bit\n",
    "15/04/18 14:43:11 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
    "15/04/18 14:43:11 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
    "15/04/18 14:43:11 INFO namenode.NNConf: ACLs enabled? false\n",
    "15/04/18 14:43:11 INFO namenode.NNConf: XAttrs enabled? true\n",
    "15/04/18 14:43:11 INFO namenode.NNConf: Maximum size of an xattr: 16384\n",
    "15/04/18 14:43:12 INFO namenode.FSImage: Allocated new BlockPoolId: BP-130729900-192.168.1.1-1429393391595\n",
    "15/04/18 14:43:12 INFO common.Storage: Storage directory /usr/local/hadoop_store/hdfs/namenode has been successfully formatted.\n",
    "15/04/18 14:43:12 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
    "15/04/18 14:43:12 INFO util.ExitUtil: Exiting with status 0\n",
    "15/04/18 14:43:12 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
    "/************************************************************\n",
    "SHUTDOWN_MSG: Shutting down NameNode at laptop/192.168.1.1\n",
    "************************************************************/\n",
    "```\n",
    "Note that hadoop namenode -format command should be executed once before we start using Hadoop. \n",
    "If this command is executed again after Hadoop has been used, it'll destroy all the data on the Hadoop file system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Hadoop\n",
    "Now it's time to start the newly installed single node cluster. \n",
    "We can use start-all.sh or (start-dfs.sh and start-yarn.sh)\n",
    "\n",
    "```Bash\n",
    "k@laptop:~$ cd /usr/local/hadoop/sbin\n",
    "\n",
    "k@laptop:/usr/local/hadoop/sbin$ ls\n",
    "distribute-exclude.sh    start-all.cmd        stop-balancer.sh\n",
    "hadoop-daemon.sh         start-all.sh         stop-dfs.cmd\n",
    "hadoop-daemons.sh        start-balancer.sh    stop-dfs.sh\n",
    "hdfs-config.cmd          start-dfs.cmd        stop-secure-dns.sh\n",
    "hdfs-config.sh           start-dfs.sh         stop-yarn.cmd\n",
    "httpfs.sh                start-secure-dns.sh  stop-yarn.sh\n",
    "kms.sh                   start-yarn.cmd       yarn-daemon.sh\n",
    "mr-jobhistory-daemon.sh  start-yarn.sh        yarn-daemons.sh\n",
    "refresh-namenodes.sh     stop-all.cmd\n",
    "slaves.sh                stop-all.sh\n",
    "\n",
    "k@laptop:/usr/local/hadoop/sbin$ sudo su hduser\n",
    "\n",
    "hduser@laptop:/usr/local/hadoop/sbin$ start-all.sh\n",
    "hduser@laptop:~$ start-all.sh\n",
    "This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\n",
    "15/04/18 16:43:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Starting namenodes on [localhost]\n",
    "localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-laptop.out\n",
    "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-laptop.out\n",
    "Starting secondary namenodes [0.0.0.0]\n",
    "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-laptop.out\n",
    "15/04/18 16:43:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "starting yarn daemons\n",
    "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-laptop.out\n",
    "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-laptop.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if it's really up and running:\n",
    "\n",
    "```Bash\n",
    "hduser@laptop:/usr/local/hadoop/sbin$ jps\n",
    "9026 NodeManager\n",
    "7348 NameNode\n",
    "9766 Jps\n",
    "8887 ResourceManager\n",
    "7507 DataNode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Hadoop\n",
    "\n",
    "```Bash\n",
    "$ pwd\n",
    "/usr/local/hadoop/sbin\n",
    "\n",
    "$ ls\n",
    "distribute-exclude.sh  httpfs.sh                start-all.sh         start-yarn.cmd    stop-dfs.cmd        yarn-daemon.sh\n",
    "hadoop-daemon.sh       mr-jobhistory-daemon.sh  start-balancer.sh    start-yarn.sh     stop-dfs.sh         yarn-daemons.sh\n",
    "hadoop-daemons.sh      refresh-namenodes.sh     start-dfs.cmd        stop-all.cmd      stop-secure-dns.sh\n",
    "hdfs-config.cmd        slaves.sh                start-dfs.sh         stop-all.sh       stop-yarn.cmd\n",
    "hdfs-config.sh         start-all.cmd            start-secure-dns.sh  stop-balancer.sh  stop-yarn.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Node\n",
    "\n",
    "http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/\n",
    "\n",
    "### reference \n",
    "http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
